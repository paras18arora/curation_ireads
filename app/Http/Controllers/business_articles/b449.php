<link rel="stylesheet" type="text/css" href="abc.css"><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><h1>Getting In On The Deep Learning Hype</h1>
				
					<img itemprop="image" itemscope itemtype="https://schema.org/ImageObject" alt="Getting In On The Deep Learning Hype" src="https://assets.entrepreneur.com/content/16x9/822/20160701060009-DeepDream.jpeg" rel="share" />

					
							
								
				
				
										
					
	
	<p>If you read tech/culture magazines, watch videos of tech events or talk to businesses about innovation, you’ll know that deep learning and artificial intelligence are <em>everywhere</em>. </p><p>But if artificial intelligence is still poorly understood, deep learning is positively shrouded in a fog of academic mystery. </p><p>Why? For one, AI as a whole has popular applications (robots, personal assistants in phones and home devices, driverless cars), but deep learning as a concept mostly has academic diagrams that look like this:</p><p><img alt="" src="https://cdn-images-1.medium.com/max/800/1*iNCOyHNUWmHRk2CkxCZk2g.png" style="height:222px; width:558px" /></p><p>While these high-level drawings may form the lingua franca of the academic world, it’s not one to capture the imagination of the wider public. So a lot of companies who use deep learning architectures, instead, attempt to explain it like this:</p><p><img alt="" src="https://cdn-images-1.medium.com/max/800/1*K9JPJQVpKxZO8Qf6vvFqEw.png" style="height:196px; width:558px" /></p><p>This is easier to follow. There’s an input image which goes through several layers of computation, which finally gives us a classification. Here, the system recognizes the object in the picture as a bird.</p><p>But there are still problems with this representation. </p><p>The biggest problem is that it does nothing to help people understand how profound deep learning can be, or why it’s changing the way we live. </p><p>Sundar Pitchai says that we’re evolving in computing from a ‘mobile-first’ to an ‘AI-first’ world. Mark Zuckerberg believes AI will overtake human perception in 5–10 years. Both companies say that deep learning is becoming the core of everything they do. </p><p>But before we get to the why, it’s important to understand what deep learning is. Because if we want to make effective business decisions in the current<a class="markup--anchor markup--p-anchor" data-href="https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1#.uik4n1whk" style="color: inherit; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.439216); background-image: linear-gradient(rgba(0, 0, 0, 0) 50%, rgba(0, 0, 0, 0.6) 50%); background-color: transparent; background-size: 2px 2px; background-position: 0px 22px; background-repeat: repeat-x;">machine intelligence landscape</a>, talk about tech or even keep up with tech that’s becoming exponentially better every passing year, we need to be able to explain how deep learning works in simple, effective ways.</p><h3>Understanding the Basics</h3><p>Every complex object is made up of a number of features. To truly define an object, we need to identify these features and find the patterns that connect them.</p><p>For example, a cube can be representationally broken down into edges, edge locations and the patterns in which they’re connected.</p><p>The human brain does this very well. It has several ‘levels’ of neurons. At each level, a complex sensory input like a sound or an image is broken down into abstractions and representations. In effect, each neural layer is a feature extractor. When the brain finally ‘recognizes’ the input and the context around it, we react accordingly. </p><p>Deep learning systems are inspired by the mammalian brain and are algorithmically modeled after it. There are stacks of neural networks called RNNS. Each stack takes an input from the previous stack, transforms it into a more abstract version and feeds it into the next level. </p><p>As Anand (Mad Street Den Co-founder and CTO) puts it,</p><blockquote><p>“[Our brain] is extremely small and uses extremely little power. We’re trying to make equivalent electronic brains.”</p></blockquote><p>The deeper the stacks, the more complex the functions of the system and the better it can handle complicated inputs. Deep learning gets its name from these ‘deep’ stacks and it’s, simply put, one of many machine learning systems. Machine learning, in turn, is one of the set of algorithms and techniques that make up artificial intelligence.</p><p>This is a fairly standard explanation. But if you have no idea what deep learning is, it’s still an overly simplistic one. Things get more interesting when we dig in a bit deeper, and that’s where good visualizations come in. </p><h3>Playing with Visualizations </h3><p>One of the best deep learning visualizations we have today is a part of Tensor Flow — Google’s open source machine learning platform. This year, Martin Wattenberg and Fernanda Viégas gave a brilliant keynote talk called <em>‘Seeing Machines Think’</em> at the <a class="markup--anchor markup--p-anchor" data-href="/r/?url=https%3A%2F%2Fopenvisconf.com%2F" rel="nofollow" style="color: inherit; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.439216); background-image: linear-gradient(rgba(0, 0, 0, 0) 50%, rgba(0, 0, 0, 0.6) 50%); background-color: transparent; background-size: 2px 2px; background-position: 0px 22px; background-repeat: repeat-x;">OpenVis Conference</a> .</p><p>In the talk, they demonstrate how a deep learning system works with a toy neural network visualization that you can play with <a class="markup--anchor markup--p-anchor" data-href="/r/?url=http%3A%2F%2Fplayground.tensorflow.org%2F%23activation%3Dtanh%26batchSize%3D10%26dataset%3Dcircle%26regDataset%3Dreg-plane%26learningRate%3D0.03%26regularizationRate%3D0%26noise%3D0%26networkShape%3D4%2C2%26seed%3D0.92978%26showTestData%3Dfalse%26discretize%3Dfalse%26percTrainData%3D50%26x%3Dtrue%26y%3Dtrue%26xTimesY%3Dfalse%26xSquared%3Dfalse%26ySquared%3Dfalse%26cosX%3Dfalse%26sinX%3Dfalse%26cosY%3Dfalse%26sinY%3Dfalse%26collectStats%3Dfalse%26problem%3Dclassification%26initZero%3Dfalse" rel="nofollow" style="color: inherit; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.439216); background-image: linear-gradient(rgba(0, 0, 0, 0) 50%, rgba(0, 0, 0, 0.6) 50%); background-color: transparent; background-size: 2px 2px; background-position: 0px 22px; background-repeat: repeat-x;">right now</a>. </p><p>Imagine two sets of dots: blue and orange. These dots can represent any data set you want. Now your machine learning system needs to correctly identify how the blue and orange dots are distributed in an image. </p><p>Pick a fairly simple pattern: blue dots grouped together on the top right, orange on the bottom left. With just a few arrangement models (or features — a vertical and a horizontal color split together make a diagonal split possible) and a couple of hidden neural layers, it’s easy to find the pattern in the image.</p><p><img alt="" src="https://cdn-images-1.medium.com/max/800/1*K06i3iz3J10l6QAbXaLx4g.png" style="height:312px; width:558px" /></p><p>But increase the complexity of the input data where the dots are arranged in a spiral, and the same features no longer work. You need more features, more hidden layers and more neurons in each layer. </p><p><img alt="" src="https://cdn-images-1.medium.com/max/800/1*P42ezKzUelpPhY7OdYi9Yg.png" style="height:322px; width:558px" /></p><p>The connecting lines depict the confidence of the prediction that each neuron is making and they become darker or lighter over time. The system is, in essence, learning. </p><p>With the right features and hidden layers, the system can now tell where the blue dots are and where the orange dots are. </p><h3>Taking the Conversation Forward</h3><p>At Mad Street Den, we work heavily with computer vision and machine learning and it’s important for us to explain what we do with simple narratives everyday. For instance, we’ve talked about <a class="markup--anchor markup--p-anchor" data-href="https://medium.com/@madstreetden/what-is-computer-vision-really-4fb5a4c7dd8c#.acrqqy3hv" style="color: inherit; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.439216); background-image: linear-gradient(rgba(0, 0, 0, 0) 50%, rgba(0, 0, 0, 0.6) 50%); background-color: transparent; background-size: 2px 2px; background-position: 0px 22px; background-repeat: repeat-x;">what computer vision really is</a> to get to the heart of how machines ‘see’ to do wonderful things. </p><p><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*rUlcdTmEWuzW7wpOCd8YDg.png" style="height:380px; width:558px" /></p><p>We’re also continually exploring alternate methods inspired by neuroscience and neuromorphic principles which we believe will help overcome some of the limitations of deep learning as we know it today. So it’s equally crucial for us to encourage and build deep learning conversations within developer communities. </p><p>The standard of public discourse around artificial intelligence has been rising over the years as developers dive in to experiment and build real-world applications with deep tech. Open source activity around deep learning is buzzing with life. And the world is sitting up and paying attention.</p><p>We talked to HasGeek which is hosting India’s first Deep Learning Conference this week in Bangalore and they’ve been seeing incredibly high interest levels around the topic:</p><blockquote><p>“We received more good talk proposals than we could fit in a day. We thought we were over-provisioning our seating capacity for the conference, but it may not be enough…and we’re at almost 3X capacity for the workshops.”</p></blockquote><p>It’s no coincidence. The more people get excited about deep learning, the better AI products get in the real world. Entrepreneur, engineer or enthusiast, it’s time we all become machine learning-literate and take the conversation — and our future — forward.</p>

	
				
				
									<div class="entnative" data-type="article-footer-promo"></div>

										
					
							